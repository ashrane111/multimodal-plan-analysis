import os
import pandas as pd
from PIL import Image
import torch
from torch.utils.data import Dataset
from transformers import LayoutLMv3Processor

class FloorPlanDataset(Dataset):
    def __init__(self, parsed_csv_path, image_dir, processor=None):
        """
        Args:
            parsed_csv_path (str): Path to the CSV generated by svg_parser.py
            image_dir (str): Path to the raw data folder containing sample folders
            processor (LayoutLMv3Processor): Hugging Face processor for tokenization/normalization
        """
        self.data = pd.read_csv(parsed_csv_path)
        self.image_dir = image_dir
        self.processor = processor
        
        # Get unique sample IDs
        self.sample_ids = self.data['sample_id'].unique()
        
        # Create a label mapping (Class Name -> ID)
        self.labels = self.data['label'].unique().tolist()
        self.label2id = {label: idx for idx, label in enumerate(self.labels)}
        self.id2label = {idx: label for label, idx in self.label2id.items()}
        
    def __len__(self):
        return len(self.sample_ids)

    def __getitem__(self, idx):
        sample_id = self.sample_ids[idx]
        
        # 1. Load Image
        img_path = os.path.join(self.image_dir, sample_id, "F1_scaled.png")
        image = Image.open(img_path).convert("RGB")
        
        # 2. Get Annotations for this sample
        sample_data = self.data[self.data['sample_id'] == sample_id]
        
        # LayoutLM expects lists of words (labels here) and boxes
        # In a real doc, "words" are OCR text. In floor plans, "words" are room names.
        words = sample_data['label'].tolist()
        
        # Ensure boxes are integer lists
        # The CSV reads them as strings "[100, 100...]", we need to eval them back to lists
        boxes = [eval(b) if isinstance(b, str) else b for b in sample_data['bbox'].tolist()]
        
        # Map string labels to integer IDs for training
        ner_tags = [self.label2id[label] for label in words]
        
        # 3. Use the Processor (The "Multi-Modal Fusion" step)
        # This handles image resizing, text tokenization, and box normalization automatically.
        encoding = self.processor(
            image,
            words,
            boxes=boxes,
            word_labels=ner_tags,
            truncation=True,
            padding="max_length", 
            max_length=512,
            return_tensors="pt"
        )
        
        # Squeeze to remove batch dimension added by processor (since we are in Dataset)
        return {
            "pixel_values": encoding["pixel_values"].squeeze(),
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "bbox": encoding["bbox"].squeeze(),
            "labels": encoding["labels"].squeeze()
        }

# Quick test block to verify it works
if __name__ == "__main__":
    # We use the pre-trained processor from Microsoft
    processor = LayoutLMv3Processor.from_pretrained("microsoft/layoutlmv3-base", apply_ocr=False)
    
    dataset = FloorPlanDataset(
        parsed_csv_path="data/processed/parsed_layout.csv",
        image_dir="data/raw",
        processor=processor
    )
    
    print(f"Dataset size: {len(dataset)}")
    print(f"Label Mapping: {dataset.label2id}")
    
    sample = dataset[0]
    print("Keys in sample:", sample.keys())
    print("Image Tensor Shape:", sample["pixel_values"].shape)
    print("BBox Tensor Shape:", sample["bbox"].shape)